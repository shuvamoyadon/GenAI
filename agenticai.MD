# -*- coding: utf-8 -*-
"""agenticAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sx9XNCq3xVj4qsxGJV7HvahiNBGjAx83

#1. What exactly defines an AI agent and how is it different from other LLM applications?

Think of it like this:

A normal LLM app (like ChatGPT answering a question) is like a calculator â€” you input something, it gives you one answer, and thatâ€™s it.

An AI agent is like a virtual assistant (like Jarvis from Iron Man). It can make decisions, use tools, and take multiple steps to complete a task.
âœ… Example:

Normal LLM app:

You ask: â€œWhatâ€™s the weather in Paris?â€
It replies: â€œItâ€™s 25Â°C and sunny.â€
AI Agent:

You ask: â€œShould I pack an umbrella for my Paris trip this weekend?â€
It:
Checks the weather for the weekend.
Analyzes the forecast.
Replies: â€œYes, thereâ€™s a 70% chance of rain on Saturday. Pack an umbrella.â€


#2. How do language models achieve autonomy in decision-making processes?
AI agents become â€œsmartâ€ by using these features:

Memory: It remembers what you said earlier.

Planning: It breaks a big task into small steps.

Tool use: It can Google things, do math, or call APIs.

Loops: It can think, act, and check again until it gets the right answer.

âœ… Example:

You ask: â€œFind me the cheapest flight from Delhi to London this weekend.â€

An AI agent might:

Search flight websites using an API.
Compare prices.
Check dates.
Say: â€œThe cheapest flight is $450 on Air India, departing Saturday night.â€


#3. What's the critical difference between agent workflows and truly autonomous agents?

Agent workflow = Like a recipe. The steps are already written.

Autonomous agent = Like a chef. It figures out the recipe based on the ingredients and goal.

âœ… Example:

You ask both to â€œwrite and publish a blog about AI trends.â€

Workflow agent:

1. Writes draft
2. Runs grammar check
3. Publishes to website

(You told it what to do.)

Autonomous agent:
1. Asks: What topic is trending?
2. Decides to write about â€œAI in Educationâ€
3. Writes it
4. Adds images
5. Publishes it

(You only gave the goal â€” it figured out everything else.)

#4. How can you integrate tools with LLMs to create effective agent systems?

Effective agent systems often rely on tool integration for real-world capability. Typical tool types:

APIs (e.g., weather, finance)
Python functions (for computation, file I/O)
Databases (retrieval-augmented generation - RAG)
Browsers or Search tools (real-time info)
Vector DBs (for semantic search & memory)
Ways to integrate:

LangChain tool wrappers
OpenAI function calling
HuggingFace tools + pipelines
Custom ReAct / Plan-and-Execute strategies
LangGraph nodes with tool execution

#5. What architectural patterns should you consider when designing AI agents?


| Pattern         | Simple Explanation                        | Example                                                       |
|----------------|--------------------------------------------|---------------------------------------------------------------|
| ReAct           | Think, then act, then check result         | "What's the capital of Brazil?" â†’ Think â†’ Google â†’ Answer     |
| Plan-and-Execute| First make a plan, then do steps           | "Plan my day" â†’ Creates a to-do list â†’ Schedules meetings     |
| LangGraph FSM   | Like a flowchart with steps & decisions    | "Get a quote" â†’ "Verify source" â†’ "Save it"                   |
| Multi-Agent     | Many small agents working together         | One agent writes, one edits, one posts the blog               |
| RAG             | Uses a database to find facts before answering | "Who is CEO of Telstra?" â†’ Searches company DB â†’ Answers |



âœ… Example using LangGraph:

A LangGraph agent may:

Take your question,
Go to a search node,
Move to processing node,
Then to output node with final answer.
"""

# Hugging Face API Token
HUGGINGFACEHUB_API_TOKEN=""
HUGGINGFACE_MODEL_ID="HuggingFaceH4/zephyr-7b-alpha"

from huggingface_hub import login
login(token=HUGGINGFACEHUB_API_TOKEN)

!pip install --upgrade --quiet transformers accelerate
!pip install bitsandbytes accelerate

"""
Demonstration of a simple multiâ€‘step workflow using Hugging Face transformers.

In a multiâ€‘step workflow, the output of one language model call becomes the
input to the next.  This technique is often called *prompt chaining* and is
commonly used for tasks that can be decomposed into fixed subtasksã€804865909491372â€ L168-L181ã€‘.
By chaining together summarization, translation and sentiment analysis,
this example shows how to build a small pipeline where each step augments the
result of the previous one.

NOTE: Running this script requires the ``transformers`` library and internet
access to download the pretrained models.  See the Hugging Face
documentation for installation details and model usage.
"""

from __future__ import annotations
import torch
from typing import Dict

try:
    from transformers import pipeline
except ImportError as e:
    raise ImportError(
        "transformers is not installed. Please install it with `pip install transformers` "
        "to run this module."
    ) from e


class MultiStepWorkflow:
    """Implements a simple threeâ€‘step LLM workflow.

    Steps:
      1. **Summarization** â€“ Condense a long piece of text into a concise summary.
      2. **Translation** â€“ Translate the English summary into French.
      3. **Sentiment Analysis** â€“ Classify the sentiment of the summary.

    The design follows the promptâ€‘chaining paradigm, where the output of
    one model feeds directly into the nextã€804865909491372â€ L168-L181ã€‘.
    """

    def __init__(self) -> None:
        # Use GPU (device=0) if available, else fallback to CPU (-1)
        device = 0 if torch.cuda.is_available() else -1

        self.summarizer = pipeline(
            "summarization",
            model="facebook/bart-large-cnn",
            device=device
        )
        self.translator = pipeline(
            "translation_en_to_fr",
            model="Helsinki-NLP/opus-mt-en-fr",
            device=device
        )
        self.classifier = pipeline(
            "sentiment-analysis",
            device=device
        )

    def run(self, text: str) -> Dict[str, str]:
        """Execute the multiâ€‘step workflow on the given text.

        Parameters
        ----------
        text : str
            The input document to process.

        Returns
        -------
        Dict[str, str]
            A dictionary containing the summary, its French translation, and
            the sentiment analysis result.
        """
        # Step 1: summarize the input text
        summary_output = self.summarizer(
            text,
            max_length=80,
            min_length=30,
            do_sample=False,
        )
        summary = summary_output[0]["summary_text"]

        # Step 2: translate the summary into French
        translation_output = self.translator(summary)
        translation = translation_output[0]["translation_text"]

        # Step 3: classify the sentiment of the summary
        sentiment_output = self.classifier(summary)
        sentiment = sentiment_output[0]["label"]

        return {
            "summary": summary,
            "translation": translation,
            "sentiment": sentiment,
        }


def main() -> None:
    workflow = MultiStepWorkflow()
    text = (
        "Artificial intelligence is transforming industries around the world. "
        "Large language models enable developers to build applications that can "
        "write articles, answer questions, translate languages and more. "
        "However, many tasks require chaining multiple calls together to achieve "
        "the desired result. In this example we will summarize this passage, "
        "translate the summary into French, and then classify its sentiment."
    )
    results = workflow.run(text)
    print("Summary:\n", results["summary"])
    print("\nTranslation (French):\n", results["translation"])
    print("\nSentiment:", results["sentiment"])


if __name__ == "__main__":
    main()

"""
Illustrative examples for building effective agents with LLMs.

This module defines several small demonstrations that correspond to common
questions about AI agents and multiâ€‘step workflows.  It contrasts simple LLM
applications with agentic systems, shows how an agent can make decisions
autonomously, demonstrates the difference between predefined workflows and
autonomous agentsã€804865909491372â€ L84-L91ã€‘, integrates external tools and outlines
an architectural pattern for an agent with memory and planning components.

Each function or class is selfâ€‘contained and can be run independently.  You
should have the ``transformers`` library installed and Internet access to
download the example models.
"""

from __future__ import annotations

from typing import Any, Callable, Dict, List, Optional

try:
    from transformers import pipeline
except ImportError as e:
    raise ImportError(
        "The transformers package is required for these examples. Install it via `pip install transformers`."
    ) from e


def simple_llm_application(text: str) -> str:
    """A basic LLM application that performs a single task.

    This function summarizes the input text using a Hugging Face pipeline.  It
    illustrates the simplest form of a language model application â€” a single
    call that returns a result.  There is no decision making or tool use.

    Parameters
    ----------
    text : str
        The text to summarize.

    Returns
    -------
    str
        The summary of the input text.
    """
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    summary = summarizer(text, max_length=80, min_length=30, do_sample=False)[0][
        "summary_text"
    ]
    return summary


class BasicAgent:
    """A simple agent that routes tasks to different pipelines.

    Unlike the singleâ€‘call example above, this agent parses user commands and
    chooses between summarization, translation or sentiment analysis.  It
    exhibits a minimal degree of autonomy: the agent decides which model to
    invoke based on the input (a simple keyword match).  This illustrates how
    agents differ from basic LLM applicationsã€804865909491372â€ L84-L91ã€‘.
    """

    def __init__(self) -> None:
        self.summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
        self.translator = pipeline(
            "translation_en_to_fr", model="Helsinki-NLP/opus-mt-en-fr"
        )
        self.classifier = pipeline("sentiment-analysis")

    def run(self, prompt: str) -> Any:
        prompt_lower = prompt.lower()
        if prompt_lower.startswith("summarize:"):
            text = prompt.split(":", 1)[1].strip()
            return self.summarizer(text)[0]["summary_text"]
        if prompt_lower.startswith("translate:"):
            text = prompt.split(":", 1)[1].strip()
            return self.translator(text)[0]["translation_text"]
        if prompt_lower.startswith("classify sentiment:"):
            text = prompt.split(":", 1)[1].strip()
            return self.classifier(text)
        # default behaviour: return raw input
        return "I don't know how to handle this request."


class PredefinedWorkflow:
    """A fixed multiâ€‘step workflow (prompt chaining).

    This class demonstrates a workflow where each step is predetermined â€”
    summarization followed by sentiment analysis.  It is akin to the prompt
    chaining pattern described in the LangGraph tutorial, where each LLM call
    processes the output of the previous oneã€804865909491372â€ L168-L181ã€‘.  Because
    the sequence is fixed, this is considered a *workflow* rather than a
    full agentã€804865909491372â€ L84-L91ã€‘.
    """

    def __init__(self) -> None:
        self.summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
        self.classifier = pipeline("sentiment-analysis")

    def run(self, text: str) -> Dict[str, Any]:
        summary = self.summarizer(text, max_length=80, min_length=30, do_sample=False)[
            0
        ]["summary_text"]
        sentiment = self.classifier(summary)[0]
        return {"summary": summary, "sentiment": sentiment}


class AutonomousAgent:
    """A dynamicallyâ€‘controlled agent with simple autonomy.

    This agent decides at runtime whether to translate a summary based on the
    sentiment score.  It performs summarization, then sentiment analysis, and
    finally â€” only if the sentiment is positive â€” translates the summary into
    French.  This conditional behaviour illustrates the autonomy of agents in
    making decisions beyond a fixed workflowã€804865909491372â€ L84-L91ã€‘.
    """

    def __init__(self) -> None:
        self.summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
        self.classifier = pipeline("sentiment-analysis")
        self.translator = pipeline(
            "translation_en_to_fr", model="Helsinki-NLP/opus-mt-en-fr"
        )

    def run(self, text: str) -> Dict[str, Any]:
        summary = self.summarizer(text, max_length=80, min_length=30, do_sample=False)[
            0
        ]["summary_text"]
        sentiment_result = self.classifier(summary)[0]
        label = sentiment_result["label"]
        result = {"summary": summary, "sentiment": sentiment_result}
        # If sentiment is positive, add a translation step
        if label == "POSITIVE":
            result["translation"] = self.translator(summary)[0]["translation_text"]
        return result


def agent_with_tool(prompt: str) -> Any:
    """Agent that integrates an external arithmetic tool.

    This function defines a tiny tool (a multiplication function) and a simple
    logic that decides whether to call the tool or defer to an LLM.  If the
    prompt contains a multiplication query like "what is 3*4?", the agent
    extracts the numbers and calls the tool; otherwise it uses a summarization
    pipeline.  This demonstrates how tool integration can extend an agentâ€™s
    capabilities beyond text generation, as described in many agentic
    frameworksã€619631130380182â€ L151-L154ã€‘.
    """

    def multiply(a: int, b: int) -> int:
        return a * b

    # simple parser for multiplication queries
    import re

    match = re.match(r".*?(\d+)\s*\*\s*(\d+).*", prompt)
    if match:
        a, b = int(match.group(1)), int(match.group(2))
        return multiply(a, b)

    # fallback to summarization for nonâ€‘arithmetic prompts
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    return summarizer(prompt)[0]["summary_text"]


class PatternedAgent:
    """Illustrates an architectural pattern with memory and planning.

    This class sketches a simple agent architecture inspired by the components
    discussed in LLM agent literatureã€619631130380182â€ L90-L104ã€‘.  The agent
    maintains a short-term memory of past interactions, uses a rudimentary
    planning method to break tasks into steps, and integrates tools for
    computations.  It is deliberately simplified for educational purposes.
    """

    def __init__(self) -> None:
        # core LLM
        self.generator = pipeline("text-generation", model="gpt2", max_length=100)
        # tool: arithmetic
        self.tool: Dict[str, Callable[[int, int], int]] = {
            "multiply": lambda a, b: a * b,
        }
        # shortâ€‘term memory
        self.memory: List[str] = []

    def plan(self, prompt: str) -> List[Dict[str, Any]]:
        """Very simple planner to parse instructions.

        It looks for arithmetic operations and splits the task accordingly.  In a
        real agent, planning would involve chainâ€‘ofâ€‘thought reasoning or
        algorithmic decompositionã€619631130380182â€ L142-L147ã€‘.
        """
        import re
        tasks = []
        match = re.match(r".*?(\d+)\s*\*\s*(\d+).*", prompt)
        if match:
            a, b = int(match.group(1)), int(match.group(2))
            tasks.append({"type": "tool", "name": "multiply", "args": (a, b)})
        else:
            tasks.append({"type": "generate", "text": prompt})
        return tasks

    def run(self, prompt: str) -> Any:
        # store in memory
        self.memory.append(prompt)
        steps = self.plan(prompt)
        print("step",steps)
        results = []
        for step in steps:
            if step["type"] == "tool":
                tool_name = step["name"]
                args = step["args"]
                if tool_name in self.tool:
                    results.append(self.tool[tool_name](*args))
            elif step["type"] == "generate":
                text = step["text"]
                response = self.generator(text, do_sample=True)[0]["generated_text"]
                results.append(response)
                self.memory.append(response)
        return results


def main() -> None:
    # Demonstrate the simple LLM application
    text = (
        "Language models enable developers to build applications that can perform"
        " various natural language processing tasks. In this example we will"
        " summarize this text."
    )
    print("Simple LLM application:\n", simple_llm_application(text))

    # Demonstrate the basic agent
    agent = BasicAgent()
    print("\nBasic agent summary:\n", agent.run("summarize: " + text))
    print("\nBasic agent translation:\n", agent.run("translate: Hello, world!"))
    print("\nBasic agent sentiment:\n", agent.run("classify sentiment: I love this!"))

    # Predefined workflow
    workflow = PredefinedWorkflow()
    print("\nPredefined workflow output:\n", workflow.run(text))

    # Autonomous agent
    autonomous = AutonomousAgent()
    print("\nAutonomous agent output (positive sentiment):\n", autonomous.run(text))
    negative_text = (
        "This movie was terrible. The plot was boring and the acting was mediocre."
    )
    print("\nAutonomous agent output (negative sentiment):\n", autonomous.run(negative_text))

    # Agent with tool
    print("\nAgent with tool (multiplication):\n", agent_with_tool("What is  2 * 3 ?"))
    print("\nAgent with tool (no math):\n", agent_with_tool(text))

    # Patterned agent
    patterned = PatternedAgent()
    print("\nPatterned agent arithmetic:\n", patterned.run("Compute 5 * 4"))
    print("\nPatterned agent generation:\n", patterned.run("Tell me a joke about cats"))


if __name__ == "__main__":
    main()

"""ğŸš€ 1. What Are Effective LLM Workflows?

An LLM workflow is a step-by-step process where the model:

Gets input (like a user question),
Processes the task,
Uses tools if needed,
Checks the result,
Gives a reliable answer.

# ğŸ¤– Designing Effective Workflows for LLMs (Large Language Models)

LLMs (like ChatGPT, Claude, or Mistral) are powerful, but to use them effectively in real-world applications, you need smart workflows. Hereâ€™s a simple guide.

---

## âœ… 1. What are the 5 Essential Design Patterns for Robust AI Systems?

| Pattern              | What It Means (Simple)                           | Example                                                                |
|----------------------|--------------------------------------------------|------------------------------------------------------------------------|
| **ReAct**            | Think, then act, then check                      | "What's the capital of Brazil?" â†’ Think â†’ Google â†’ Answer              |
| **Plan-and-Execute** | Make a plan first, then follow steps             | "Plan my day" â†’ List tasks â†’ Schedule meetings                         |
| **FSM (LangGraph)**  | A flowchart of fixed steps and decisions         | "Upload file" â†’ "Validate" â†’ "Store"                                   |
| **Multi-Agent**      | Several LLMs with different jobs                 | One agent writes â†’ One edits â†’ One posts to blog                       |
| **RAG**              | Use external knowledge or database to answer     | "Who is CEO of Telstra?" â†’ Search docs â†’ Generate answer               |

---

## ğŸ§ª 2. How to Implement Validation & Quality Control?

To make sure LLMs produce good output:

1. **Self-check**  
   Ask the model to critique its own output.  
   _Example_: â€œWas the answer correct and complete?â€

2. **Tool-based Validation**  
   Use code or APIs to check things.  
   _Example_: Use regex to validate an email address.

3. **Human-in-the-loop (HITL)**  
   Have humans approve critical outputs.  
   _Example_: Approve a legal summary before publishing.

4. **Voting / Redundancy**  
   Ask multiple times, compare results.  
   _Example_: Run 3 drafts and pick the best one.

---

## ğŸ—ï¸ 3. What Techniques Does Anthropic Recommend?

Anthropic recommends:

- Break problems into **clear, small tasks**
- Use **structured prompts** instead of one big one
- Let the model **review or refine its output**
- Use **transparent steps** that are easy to debug
- Add **guardrails** (rules, filters) to avoid mistakes

---

## ğŸ”— 4. How to Orchestrate Multiple LLMs Together?

You can make multiple LLMs work like a team â€” each with a specific role. This approach makes your AI system smarter, more reliable, and easier to manage.

### ğŸ”¹ Step 1: Assign Roles

Give each LLM a clear task:

- **Writer** â†’ Generates the initial content  
- **Reviewer** â†’ Improves grammar, tone, or structure  
- **Fact Checker** â†’ Verifies the correctness of information  
- **Action Agent** â†’ Executes an action like sending an email or saving to a database

---

### ğŸ”¹ Step 2: Chain Them Together

Pass the output of one LLM to the next, like a relay race.

Writer â†’ Reviewer â†’ Fact Checker â†’ Action Agent


Each step makes the result better, safer, or more accurate.

---

### ğŸ”¹ Step 3: Use a Controller

Use a central script, orchestration tool, or framework to manage the flow between LLMs.

**Example tools**:
- LangGraph (graph-based orchestration)
- CrewAI (multi-agent systems)
- Python scripts with conditional logic

---

### âœ… Example Workflow

**Goal**: Send a high-quality, personalized email using multiple LLMs.

1. **Writer Agent**: Drafts the email based on a customer support case.  
2. **Reviewer Agent**: Polishes tone and grammar.  
3. **Fact Checker Agent**: Verifies that product info is up to date.  
4. **Action Agent**: Sends the email via an API.

**Flow**: User Request â†’ Writer â†’ Reviewer â†’ Fact Checker â†’ Send Email



